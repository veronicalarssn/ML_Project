{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D, Dense, BatchNormalization, AveragePooling2D, Flatten\n",
    "from sklearn import svm \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "import aspose.words as aw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3db6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Getting df with all filenames and images (missing labels at this stage) - shape = (166866,2)\"\"\"\n",
    "# only take first ten photos in each brand folder\n",
    "# (150,150) grayscale --> 2m 28.9s\n",
    "# (150,150) rgb, with pixels as cols --> \n",
    "\n",
    "def load_images_from_folder(folder_path, image_size=(150, 150)):\n",
    "    images = []\n",
    "    \n",
    "    # Recursively traverse through the folder structure\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            # Get the full path of the file\n",
    "            file_path = os.path.join(root, filename)\n",
    "            \n",
    "            # Check if the file is an image\n",
    "            if any(filename.endswith(extension) for extension in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "                # Load the image using PIL (Python Imaging Library)\n",
    "                try:\n",
    "                    with Image.open(file_path) as img:\n",
    "                        img = img.resize(image_size) #resizing image\n",
    "                        img = img.convert('RGB') #Converting to gray scale\n",
    "                        #img_array = np.array(img) / 255.0 # Convert the image to numpy array and normalize pixel values\n",
    "                         # Extract folder name from the root path\n",
    "                        label = os.path.basename(root)\n",
    "                        \n",
    "                        images.append({'Label': label, 'Filename': filename, 'Image': img})\n",
    "                        #images.append({'Filename': filename, 'Image': img}) # Append the image array to the list\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image '{file_path}': {e}\")\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    images_df = pd.DataFrame(images)\n",
    "    return images_df\n",
    "\n",
    "# Loading\n",
    "png_folder_path=r\"C:\\Users\\emili\\OneDrive\\Dokumente\\03_Master_CBS\\05_Summerterm24\\01_ML and DL\\Project\\png logo findder\"\n",
    "image_df = load_images_from_folder(png_folder_path)\n",
    "print('Image Dataframe: \\n \\n', image_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f21bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "image_df['Label coded'] = label_encoder.fit_transform(image_df['Label'])\n",
    "image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f25a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split it into features and labels\n",
    "images = image_df.drop(['Filename','Label coded','Label'], axis=1)\n",
    "labels = image_df[['Label coded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d40790",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train ,X_test, y_train, y_test=train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform PCA on X to reduce amount of features\n",
    "# reduce to dimensionality so that 80% of variance is kept\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Convert PIL images/tensors in DataFrame to NumPy array\n",
    "X_train = np.array([np.array(img) for img in X_train.iloc[:, 0]])\n",
    "X_test = np.array([np.array(img) for img in X_test.iloc[:, 0]])\n",
    "\n",
    "# reshape features accordingly to SVM architecture NOTE: train test split again!!\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "pca = PCA(n_components=0.8, random_state=42)\n",
    "X_pca_train = pca.fit_transform(X_train)\n",
    "\n",
    "# check how many principal components are needed\n",
    "pca.n_components_\n",
    "\n",
    "# apply the trained PCA model on the test set as well\n",
    "X_pca_test = pca.transform(X_test)\n",
    "\n",
    "# check that PCA worked\n",
    "print(X_pca_train.shape)\n",
    "print(X_pca_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d55f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the linear SVM model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "svm = SVC(kernel='linear', class_weight='balanced', probability=True, random_state=42)\n",
    "multi_svm = ClassifierChain(svm)\n",
    "\n",
    "# define the hyperparameter grid\n",
    "param_grid = {\n",
    "    # the selection of the hyperparameters was in the beginning larger and reduced after some tries\n",
    "    # restricting the model with smaller values made the best results\n",
    "    'base_estimator__C':[2**-5, 2**-4, 2**-3, 2**-1],\n",
    "}\n",
    "\n",
    "# define the scoring metric\n",
    "scorer = make_scorer(roc_auc_score, multi_class='ovr', average='macro')\n",
    "\n",
    "# define the grid search\n",
    "grid = GridSearchCV(\n",
    "    multi_svm,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=scorer,\n",
    "    refit=True,\n",
    "    return_train_score=True,\n",
    "    verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfb81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the SVMs\n",
    "grid.fit(X_pca_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
