{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision\n",
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-Load the dataset\n",
    "#loaded_data = np.load(\"my_dataset_aug_full.npz\", allow_pickle=True)\n",
    "# # Access images and labels\n",
    "# loaded_images = loaded_data['images']\n",
    "\n",
    "# # Now you can use loaded_images and loaded_labels in your code\n",
    "# image_df=pd.DataFrame(loaded_images)\n",
    "\n",
    "image_df = pd.read_csv(\"/Users/veronicalarsson/Downloads/model_data.csv\")\n",
    "image_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split it into features and labels\n",
    "features = image_df.iloc[:, :51529]\n",
    "labels = image_df.iloc[:, 51530]\n",
    "labels_coded=image_df.iloc[:,51531]\n",
    "labels_coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_coded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting splits\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_pic = X_train.iloc[0]\n",
    "one_pic_array = one_pic.values.astype(float)  # Convert to float\n",
    "one_pic_image = one_pic_array.reshape((227, 227))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(one_pic_image)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data must be preprocessed before training the network. When inspecting one image in the training set, it can be observed that the pixel values fall in the range of 0 to 255.\n",
    "\n",
    "These values can be scaled to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255. It's important that the training set and the testing set be preprocessed in the same way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train/255.0\n",
    "X_test=X_test/255.0\n",
    "\n",
    "#to verify whether it worked: plot\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_train[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the neural network requires configuring the layers of the model, then compiling the model.\n",
    "\n",
    "Set up the layers\n",
    "The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. Hopefully, these representations are meaningful for the problem at hand.\n",
    "\n",
    "Most of deep learning consists of chaining together simple layers. Most layers, such as tf.keras.layers.Dense, have parameters that are learned during training.\n",
    "\n",
    "The first layer in this network, tf.keras.layers.Flatten, transforms the format of the images from a two-dimensional array (of 227 by 227 pixels) to a one-dimensional array (of 227 * 227 = 51528 pixels). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data. --< true???\n",
    "\n",
    "After the pixels are flattened, the network consists of a sequence of two tf.keras.layers.Dense layers. These are densely connected, or fully connected, neural layers. The first Dense layer has 128 nodes (or neurons). The second (and last) layer returns a logits array with length of 27. Each node contains a score that indicates the current image belongs to one of the 27 classes.\n",
    "\n",
    "## MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(227,227)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(27)\n",
    "])\n",
    "\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#To start training, call the model.fit methodâ€”so called because it \"fits\" the model to the training data:\n",
    "model_1.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "#Next, compare how the model performs on the test dataset:\n",
    "test_loss, test_acc = model_1.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "#predictions\n",
    "probability_model = tf.keras.Sequential([model_1, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "\n",
    "predictions = probability_model.predict(X_test)\n",
    "print(predictions[0])\n",
    "#A prediction is an array of 10 numbers. They represent the model's \"confidence\" that the image corresponds to each of the 27 different logos.\n",
    "#You can see which label has the highest confidence value by calling:\n",
    "print(f'most probable logo is',np.argmax(predictions[0]))\n",
    "\n",
    "#the model is most confident that this image is XX\n",
    "#Examining the test label shows that this classification is correct?False?:\n",
    "\n",
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(keras.layers.Flatten(input_shape=227,227))\n",
    "model_2.add(keras.layers.Dense(100, activation='relu'))\n",
    "model_2.add(kersa.layers.Dropout(0.2))\n",
    "model_2.add(keras.layers.Dense(27))\n",
    "\n",
    "model_2.summary()\n",
    "            \n",
    "predictions = model_2(X_train[:1]).numpy()\n",
    "predictions\n",
    "\n",
    "#Using `tf.nn.softmax` function converts these logits to *probabilities* for each class: \n",
    "tf.nn.softmax(predictions).numpy()\n",
    "\n",
    "#Defining a loss function for training using `losses.SparseCategoricalCrossentropy`:\n",
    "#This loss is equal to the negative log probability of the true class: \n",
    "#The loss is zero if the model is sure of the correct class.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "loss_fn(y_train[:1], predictions).numpy()\n",
    "\n",
    "#Before training, configuring and compiling the model\n",
    "#Setting the optimizer class to adam, the loss to the loss_fn function defined earlier\n",
    "\n",
    "model_2.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "#or as before but with sgd: model_2.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             #optimizer=\"sgd\",\n",
    "            # metrics=[\"accuracy\"]) we use sparse... bc sparse labels and classes are exclusive\n",
    "\n",
    "#training and evaluation\n",
    "history=model_2.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))\n",
    "\n",
    "#or Model.evaluate` method checks the model's performance, usually on a validation set and test set\n",
    "#model_2.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "#model to return a probability, you can wrap the trained model, and attach the softmax to it:\n",
    "probability_model = tf.keras.Sequential([model_2, tf.keras.layers.Softmax()])\n",
    "probability_model(X_test[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tryouts with subset of length 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try-outs with smaller subsets:\n",
    "X_train_subset = X_train[:100]\n",
    "y_train_subset = y_train[:100]\n",
    "X_test_subset = X_test[:100]\n",
    "y_test_subset = y_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_subset=X_train_subset/255.0\n",
    "X_test_subset=X_test_subset/255.0\n",
    "\n",
    "X_train_subset_array = X_train_subset.to_numpy()  # Convert DataFrame to NumPy array\n",
    "\n",
    "# Reshape the array\n",
    "X_train_reshaped = X_train_subset_array.reshape(100,227, 227)\n",
    "X_train_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_subset.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train_reshaped to float32\n",
    "X_train_reshaped = np.array(X_train_reshaped, dtype=np.float32)\n",
    "y_train_subset=np.array(y_train_subset, dtype=np.float32)\n",
    "\n",
    "model_1.fit(X_train_reshaped, y_train_subset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_subset_array = X_test_subset.to_numpy()\n",
    "X_test_reshaped = X_test_subset_array.reshape(100,227, 227)\n",
    "X_test_reshaped = np.array(X_test_reshaped, dtype=np.float32)\n",
    "y_test_subset=np.array(y_test_subset, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_1.evaluate(X_test_reshaped,  y_test_subset, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "probability_model = tf.keras.Sequential([model_1, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "\n",
    "predictions = probability_model.predict(X_test_reshaped)\n",
    "print(predictions[0])\n",
    "#A prediction is an array of 10 numbers. They represent the model's \"confidence\" that the image corresponds to each of the 27 different logos.\n",
    "#You can see which label has the highest confidence value by calling:\n",
    "print(f'most probable logo is',np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model is most confident that this image is 4\n",
    "#Examining the test label shows that this classification is correct?False?:\n",
    "\n",
    "y_test_subset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
